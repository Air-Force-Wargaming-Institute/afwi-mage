stages:
  - prebuild
  - security_scan
  - build_test
  - compliance
  - quality_gates
  - deploy
  - audit_trail

prebuild_checks:
  stage: prebuild
  image: python:3.10
  script:
    - set -x  # Enable command tracing for debugging
    - python3.10 --version || (echo "Python 3.10 not found" && exit 1)
    - python3.11 --version || (echo "Python 3.11 not found" && exit 1)
    # Skip NVIDIA driver check; log a warning instead.
    - command -v nvidia-smi || echo "NVIDIA drivers not installed; skipping GPU check"
    - nvidia-smi || echo "nvidia-smi unavailable; proceeding without GPU metrics"
    - mkdir -p app/data/{uploads,extraction,datasets,outputs,logs,temp_conversions,builder/AGENTS,builder/TEAMS}
    - mkdir -p app/models/{base_models,fine_tuned_models}
    - mkdir -p app/nltk_data
    - ls -ld app/data/uploads
  artifacts:
    paths:
      - app/data/logs

security_scan:
  stage: security_scan
  image: python:3.10
  script:
    - set -x
    - pip install bandit safety trufflehog
    - bandit -r . -f json -o sast_report.json || true
    - safety check --json > dependency_check.json || true
    - |
      # Check if dependency_check.json contains valid JSON; if not, write a default CSV header.
      if python -c "import json; json.load(open('dependency_check.json'))" 2>/dev/null; then 
        python -c "import json, csv; data=json.load(open('dependency_check.json')); csvfile=open('dependency_check.csv','w',newline=''); writer=csv.writer(csvfile); writer.writerow(['package','version']); [writer.writerow([pkg.get('package'), pkg.get('version')]) for pkg in data.get('vulnerabilities',[])]; csvfile.close()"
      else 
        echo "dependency_check.json is not valid JSON, skipping CSV conversion"
        echo "package,version" > dependency_check.csv
      fi
    - curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
    - trivy image --format json --output container_scan.json my-docker-image:latest || true
    - trufflehog filesystem . --json > secret_detection.json || true
  artifacts:
    paths:
      - sast_report.json
      - dependency_check.csv
      - container_scan.json
      - secret_detection.json

build_test:
  stage: build_test
  image: docker:latest
  services:
    # Note: We no longer use docker:dind since we're bind-mounting the host's Docker socket.
    - name: postgres:13
      alias: db
  variables:
    # These are standard credentials for the official Postgres image.
    # Note: Your Docker Compose file still has the db service commented out,
    # so ensure your application connects to an external/pre-configured DB if needed.
    POSTGRES_PASSWORD: password
    POSTGRES_USER: postgres
    POSTGRES_DB: dbname
  script:
    - set -x
    # List the current directory to verify the repository structure.
    - ls -la
    # Wait until the Docker socket is present (i.e. the host's Docker daemon is available).
    - until [ -S /var/run/docker.sock ]; do echo "Waiting for Docker socket..."; sleep 1; done
    # Build images using the docker-compose file located in the backend folder.
    - docker-compose -f backend/docker-compose.yml --verbose build
    - docker-compose -f backend/docker-compose.yml run --rm core pytest --verbose --junitxml=unit_tests.xml --maxfail=1 --disable-warnings
    - docker-compose -f backend/docker-compose.yml run --rm core pytest --verbose tests/integration --json-report --json-report-file=integration_tests.json
    - docker-compose -f backend/docker-compose.yml run --rm core pytest --verbose tests/performance --json-report --json-report-file=performance_metrics.json
    - docker-compose -f backend/docker-compose.yml run --rm core pytest --verbose --cov=. --cov-report=xml:coverage_report.xml
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock
  artifacts:
    paths:
      - unit_tests.xml
      - integration_tests.json
      - performance_metrics.json
      - coverage_report.xml

compliance:
  stage: compliance
  image: alpine:latest
  script:
    - set -x
    - |
      cat <<'EOF' > direct_chat_healthcheck.yaml
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8011/api/v1/health"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 10s
      EOF
    - |
      cat <<'EOF' > database_healthcheck.yaml
      healthcheck:
        test: ["CMD-SHELL", "pg_isready -U postgres"]
        interval: 5s
        timeout: 5s
        retries: 5
      EOF
    - |
      cat <<'EOF' > service_dependencies.yaml
      depends_on:
        condition: service_healthy
      EOF
  artifacts:
    paths:
      - direct_chat_healthcheck.yaml
      - database_healthcheck.yaml
      - service_dependencies.yaml

quality_gates:
  stage: quality_gates
  image: alpine:latest
  dependencies:
    - build_test
    - security_scan
  script:
    - set -x
    - |
      COVERAGE=$(grep -o 'line-rate="[^"]*' coverage_report.xml | head -1 | cut -d'"' -f2 | awk '{printf "%d", $1 * 100}')
      echo "Coverage: ${COVERAGE}%"
      if [ "$COVERAGE" -lt 80 ]; then
        echo "Error: Code coverage is below 80%"; exit 1;
      fi
    - |
      HIGH_ISSUES=$(jq '.high_severity' sast_report.json)
      echo "High severity issues: $HIGH_ISSUES"
      if [ "$HIGH_ISSUES" -gt 0 ]; then
        echo "Error: High severity security issues found"; exit 1;
      fi

deploy:
  stage: deploy
  image: docker:latest
  services:
    - docker:dind
  only:
    - main
  when: manual
  script:
    - set -x
    - docker-compose -f backend/docker-compose.yml up -d
    - echo "Deployment completed successfully"
  environment:
    name: production

audit_trail:
  stage: audit_trail
  image: alpine:latest
  dependencies:
    - prebuild_checks
  script:
    - set -x
    - tar -czf build_logs.tar.gz app/data/logs/
  artifacts:
    paths:
      - build_logs.tar.gz
