# Model Service Configuration

# Server configuration
PORT: 8008
HOST: "0.0.0.0"

# Logging
LOG_PATH: "/app/data/logs"
LOG_LEVEL: "INFO"

# Model configuration
MODEL_CACHE_PATH: "/app/data/models"
DEFAULT_MODEL: "/models/Llama-3.2-1B-Instruct-abliterated"

# API configuration
BATCH_SIZE: 8
MAX_TOKENS: 16384  # Increased to handle extremely long responses
DEFAULT_TEMPERATURE: 0.4
DEFAULT_TOP_P: 0.95
TIMEOUT: 120  # Extended timeout (5 minutes) for very long generations

# External services
VLLM_API_URL: "http://host.docker.internal:8007/v1"  # vLLM OpenAI-compatible API
EMBEDDING_SERVICE_URL: "http://host.docker.internal:8006"

# Security configuration 
ENABLE_AUTH: False  # Set to True to enable authentication
API_KEY_HEADER: "X-API-Key" 