service:
  name: direct_chat_service
  host: 0.0.0.0
  port: 8011
  debug: true

vllm:
  chat_completion_url: http://host.docker.internal:8007/v1
  chat_model: /models/DeepHermes-3-Llama-3-8B-Preview
  max_tokens: 4096
  temperature: 0.4
  timeout: 300

ollama:
  # Base URL for Ollama API
  base_url: http://host.docker.internal:11434
  # Embedding configuration
  embedding_url: http://host.docker.internal:11434
  embedding_model: nomic-embed-text:latest
  timeout: 300
  
  # Model identifier for Ollama
  ollama_model: hf.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF:latest
  
  # Controls randomness in the output (range: 0.0-1.0)
  # Higher values = more creative, lower = more focused
  temperature: 0.2
  
  # Maximum number of tokens in context window
  # Determines how much previous conversation the model can see
  context_window: 20000
  
  # Nucleus sampling, cumulative probability cutoff (range: 0.0-1.0)
  # Lower values = more focused on likely tokens
  top_p: 0.9
  
  # Limits number of tokens considered for sampling (range: 1-100)
  # Lower values = more focused responses
  top_k: 40
  
  # Penalizes repeated tokens (range: 1.0-2.0)
  # Higher values reduce repetition more strongly
  repeat_penalty: 1.1
  
  # GPU-specific settings
  num_gpu: 99  # Use all available GPU layers
  num_thread: 1  # Minimize CPU threading
  f16: true  # Use half-precision for better GPU performance
  
  # Sequences where the model should stop generating
  # Useful for maintaining conversation format
  stop: ["Human:", "Assistant:"]

api:
  core_service_url: http://core:8000
  prefix: /api/direct_chat

cors:
  allowed_origins:
    - http://localhost:3000
    - http://127.0.0.1:3000
    - "*"
  allowed_methods:
    - GET
    - POST
    - OPTIONS
  allowed_headers:
    - Content-Type
    - Authorization

chat_logging:
  enabled: true
  base_path: "sessions"
  history_window: 100  # Number of messages to keep in memory
  max_token_window: 20000  # Maximum tokens to include in context
  max_new_tokens: 2500

server:
  host: "0.0.0.0"  # Listen on all available interfaces
  port: 8011         # Port for the Direct Chat service
  workers: 4         # Number of worker processes (adjust based on CPU cores)
  reload: false      # Set to true for development auto-reload
  log_level: "info"  # Logging level (debug, info, warning, error, critical)
