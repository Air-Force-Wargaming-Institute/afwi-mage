name: afwi-multi-agent-generative-engine

networks:
  app-network:
    driver: bridge

services:
  init-data:
    image: busybox
    volumes:
      - ../data:/data
    command: >
      sh -c "mkdir -p /data/uploads && chmod -R 777 /data/uploads"

  core:
    build: ./core_service
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/dbname
      - UPLOAD_SERVICE_URL=http://upload:8005
      - EXTRACTION_SERVICE_URL=http://extraction:8002
      - GENERATION_SERVICE_URL=http://generation:8003
      - AGENT_SERVICE_URL=http://agent:8001
      - REVIEW_SERVICE_URL=http://review:8004
      - EMBEDDING_SERVICE_URL=http://embedding:8006
      - DEBUG=1
      - PYTHONUNBUFFERED=1
    volumes:
      - ../data:/app/data:rw
    depends_on:
      init-data:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - app-network

  chat:
    build: ./chat_service
    ports:
      - "8009:8009"
    environment:
      - CORE_SERVICE_URL=http://core:8000
    volumes:
      - ../data:/app/data
      - ./chat_service:/app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - init-data
    networks:
      - app-network

  agent:
    build: ./agent_service
    ports:
      - "8001:8001"
    volumes:
      - ../data:/app/data
      - ./models:/app/models
      - ./agent_service:/app
      - ./agent_service/agents:/app/agents
    environment:
      - PYTHONPATH=/app
      - CORE_SERVICE_URL=http://core:8000
    depends_on:
      - init-data
    networks:
      - app-network

  extraction:
    build: ./extraction_service
    ports:
      - "8002:8002"
    volumes:
      - ../data:/app/data
      - ./models:/app/models
      - ./extraction_service:/app
      - nltk_data_volume:/app/nltk_data
    depends_on:
      - init-data
    networks:
      - app-network

  generation:
    build: ./generation_service
    ports:
      - "8003:8003"
    volumes:
      - ../data:/app/data
      - ./models:/app/models
      - ./generation_service:/app
    environment:
      - CORE_SERVICE_URL=http://core:8000
    depends_on:
      - init-data
    networks:
      - app-network

  review:
    build: ./review_service
    ports:
      - "8004:8004"
    volumes:
      - ../data:/app/data
      - ../models:/app/models
      - ./review_service:/app
    environment:
      - CORE_SERVICE_URL=http://core:8000
    depends_on:
      - init-data
    networks:
      - app-network

  upload:
    build: ./upload_service
    ports:
      - "8005:8005"
    volumes:
      - ../data:/app/data
      - ./models:/app/models
      - ./upload_service:/app
    environment:
      - CORE_SERVICE_URL=http://core:8000
    depends_on:
      - init-data
    networks:
      - app-network

  embedding:
    build: ./embedding_service
    ports:
      - "8006:8006"
    volumes:
      - ../data:/app/data
      - ./models:/app/models
      - ./embedding_service:/app
    environment:
      - CORE_SERVICE_URL=http://core:8000
      - API_KEY='None'
      - BASE_URL='None'
    depends_on:
      - init-data
    networks:
      - app-network

  direct_chat:
    build: ./direct_chat_service
    ports:
      - "8011:8011"
    volumes:
      - ../data:/app/data
      - ./direct_chat_service:/app
    environment:
      - CORE_SERVICE_URL=http://core:8000
      - EMBEDDING_SERVICE_URL=http://embedding:8006
      - MODEL_SERVICE_URL=http://model_service:8008
      - PYTHONPATH=/app
      - SERVICE_PORT=8011
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - init-data
      - embedding
    networks:
      - app-network
    command: uvicorn app:app --host 0.0.0.0 --port 8011 --reload --timeout-keep-alive 75 --limit-concurrency 100 --backlog 100

  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8007:8000"
    environment:
      - MAX_NUM_BATCHED_TOKENS=4096
      - MAX_NUM_SEQS=256
      - MAX_PADDING_LENGTH=256
      - MAX_MODEL_LEN=4096
      - QUANTIZATION=awq
      - TRUST_REMOTE_CODE=true
      - HF_DATASETS_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
      - HF_HUB_OFFLINE=1
      - GPU_MEMORY_UTILIZATION=0.85
    volumes:
      - ../models:/models
      - ../data:/data
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: --model /models/DeepHermes-3-Llama-3-8B-Preview --tokenizer /models/DeepHermes-3-Llama-3-8B-Preview --dtype=half --tensor-parallel-size=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l) --gpu-memory-utilization=0.85 --max-model-len=65536

  vllm_embeddings:
    image: vllm/vllm-openai:latest
    ports:
      - "8012:8000"
    environment:
      - MAX_NUM_BATCHED_TOKENS=4096
      - MAX_NUM_SEQS=256
      - MAX_PADDING_LENGTH=256
      - MAX_MODEL_LEN=4096
      - QUANTIZATION=awq
      - TRUST_REMOTE_CODE=true
      - HF_DATASETS_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
      - HF_HUB_OFFLINE=1
      - GPU_MEMORY_UTILIZATION=0.10
    volumes:
      - ../models:/models
      - ../data:/data
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: --model /models/bge-base-en-v1.5 --tokenizer /models/bge-base-en-v1.5 --dtype=half --tensor-parallel-size=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l) --gpu-memory-utilization=0.10

  frontend:
    build: ../frontend
    ports:
      - "80:80"
    environment:
      - NODE_ENV=production
      - PORT=80
    volumes:
      - ../data:/data
      - ../frontend:/app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - app-network

volumes:
  postgres_data:
  nltk_data_volume:

